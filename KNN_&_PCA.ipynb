{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment"
      ],
      "metadata": {
        "id": "8hh64YMngsC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks.\n",
        "It is a non-parametric, instance-based (lazy) algorithm — meaning it makes predictions directly from the training data without learning a fixed model.\n",
        "\n",
        "**KNN for Classification :**\n",
        "\n",
        "- Each of the K nearest neighbors votes for its class.\n",
        "\n",
        "- The majority class among these neighbors becomes the predicted class.\n",
        "\n",
        "**KNN for Regression :**\n",
        "\n",
        "- The algorithm takes the average (or weighted average) of the target values of the K nearest neighbors."
      ],
      "metadata": {
        "id": "OnwZSISTgsAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.2. : What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "The Curse of Dimensionality refers to the problems that arise when working with high-dimensional data — that is, data with many features (variables).\n",
        "\n",
        "As the number of dimensions increases:\n",
        "\n",
        "- The data becomes sparse (spread out).\n",
        "\n",
        "- Distances between points become less meaningful.\n",
        "\n",
        "- The computational cost increases drastically.\n",
        "\n",
        "**How It Affects KNN Performance**\n",
        "\n",
        "KNN relies heavily on distance metrics (like Euclidean distance).\n",
        "When dimensions increase:\n",
        "\n",
        "- Nearest neighbors aren’t truly “close” anymore\n",
        "- Model has fewer relevant neighbors to learn from\n",
        "- Noise dominates because similar points are rare\n",
        "- Distance computation becomes slow and memory-heavy"
      ],
      "metadata": {
        "id": "dF3YHLWFgr9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.3. What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique that transforms a large set of correlated features into a smaller set of uncorrelated features called principal components — while preserving as much variance (information) as possible.\n",
        "\n",
        "Different from feature selection :    \n",
        "\n",
        "PCA → Creates new transformed features (reduces dimensionality by feature extraction).\n",
        "\n",
        "Feature Selection → Chooses the best original features (reduces dimensionality by elimination)."
      ],
      "metadata": {
        "id": "tKCdm4CDgr63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4.  What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "An eigenvector is a direction in which a linear transformation (like PCA’s covariance matrix) acts by stretching or compressing the data.\n",
        "\n",
        "The corresponding eigenvalue tells how much variance (information) is captured along that eigenvector’s direction.\n",
        "\n",
        "why are they important :     \n",
        "\n",
        "1. Determine principal components:\n",
        " The eigenvectors define the new axes (principal components) of the reduced feature space.\n",
        "\n",
        "2. Measure importance (variance explained):\n",
        "Eigenvalues tell how much variance (information) each principal component retains.\n",
        "\n",
        "3. Dimensionality reduction:\n",
        "By choosing the top k eigenvectors (those with largest eigenvalues), we can represent most of the information with fewer dimensions.\n",
        "\n",
        "4. Noise reduction:\n",
        "Small eigenvalues correspond to components with little variance (often noise), which can be discarded."
      ],
      "metadata": {
        "id": "tE_NyItBgr39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.5. How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "1. Apply PCA :\n",
        "\n",
        "- Process :\n",
        "Reduce the dimensionality of the data by transforming it into a smaller number of principal components.\n",
        "- Benefits : Removes noise, reduces feature redundancy, and simplifies distance calculations.\n",
        "2. Apply KNN :\n",
        "- Process : Perform classification or regression in the lower-dimensional PCA space.\n",
        "- Benefits : Improves efficiency and accuracy by making distances more meaningful and reducing overfitting."
      ],
      "metadata": {
        "id": "3nckuiFVgr0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset:\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "\n",
        "Q.6. Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "\n",
        "\n",
        "Answer ->>"
      ],
      "metadata": {
        "id": "lB2PELyfgryH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# 2. Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ----------------------------\n",
        "# Case 1: Without Feature Scaling\n",
        "# ----------------------------\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_no_scale = knn.predict(X_test)\n",
        "acc_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "# ----------------------------\n",
        "# Case 2: With Feature Scaling\n",
        "# ----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Compare Results\n",
        "# ----------------------------\n",
        "print(f\"Accuracy without Feature Scaling: {acc_no_scale:.4f}\")\n",
        "print(f\"Accuracy with Feature Scaling:    {acc_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0izN9VFW7ux",
        "outputId": "453ec419-8cac-421f-de1d-dfb00b4e92a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Feature Scaling: 0.7407\n",
            "Accuracy with Feature Scaling:    0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.7. Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "\n",
        "Answer ->>"
      ],
      "metadata": {
        "id": "9ojXMZ8cgrvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Standardize the features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 4. Print explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Display in a nice table\n",
        "df_variance = pd.DataFrame({\n",
        "    'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance))],\n",
        "    'Explained Variance Ratio': explained_variance\n",
        "})\n",
        "\n",
        "print(df_variance)\n",
        "print(\"\\nTotal Variance Explained:\", explained_variance.sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PolFhp00XIRp",
        "outputId": "6683f307-aa1e-44a8-d13f-bb800b89ea42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Principal Component  Explained Variance Ratio\n",
            "0                  PC1                  0.361988\n",
            "1                  PC2                  0.192075\n",
            "2                  PC3                  0.111236\n",
            "3                  PC4                  0.070690\n",
            "4                  PC5                  0.065633\n",
            "5                  PC6                  0.049358\n",
            "6                  PC7                  0.042387\n",
            "7                  PC8                  0.026807\n",
            "8                  PC9                  0.022222\n",
            "9                 PC10                  0.019300\n",
            "10                PC11                  0.017368\n",
            "11                PC12                  0.012982\n",
            "12                PC13                  0.007952\n",
            "\n",
            "Total Variance Explained: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n",
        "\n",
        "Answer ->>"
      ],
      "metadata": {
        "id": "Kl2Dtai4XMSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# 2. Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# 3. Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Case 1: KNN on Original Scaled Data\n",
        "# ------------------------------------------------------\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn.predict(X_test_scaled)\n",
        "acc_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Case 2: PCA Transformation (retain top 2 components)\n",
        "# ------------------------------------------------------\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# 4. Compare Results\n",
        "# ------------------------------------------------------\n",
        "print(f\"Accuracy on Original (Scaled) Dataset: {acc_original:.4f}\")\n",
        "print(f\"Accuracy on PCA-Reduced Dataset (2 Components): {acc_pca:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIf1tqCAXgiY",
        "outputId": "93c03c99-8782-4dab-8cff-0e4a0709c618"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Original (Scaled) Dataset: 0.9444\n",
            "Accuracy on PCA-Reduced Dataset (2 Components): 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9. Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "Answer ->>"
      ],
      "metadata": {
        "id": "Bf1E__t4Xjag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# 2. Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Scale features (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Case 1: KNN with Euclidean distance (default)\n",
        "# -----------------------------------------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Case 2: KNN with Manhattan distance\n",
        "# -----------------------------------------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# -----------------------------------------------\n",
        "# 4. Compare Results\n",
        "# -----------------------------------------------\n",
        "print(f\"Accuracy (Euclidean distance): {acc_euclidean:.4f}\")\n",
        "print(f\"Accuracy (Manhattan distance): {acc_manhattan:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEBgEJd4XrRa",
        "outputId": "1e94546f-5f74-4cff-80b2-6490600041df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Euclidean distance): 0.9444\n",
            "Accuracy (Manhattan distance): 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.10. You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for       real-world biomedical data\n",
        "\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "1. Use PCA for Dimensionality Reduction\n",
        "\n",
        "    - Standardize data, then apply PCA.\n",
        "\n",
        "    - PCA compresses correlated gene features into fewer uncorrelated components → removes noise & redundancy.\n",
        "\n",
        "2. Decide Number of Components\n",
        "\n",
        "    - Use cumulative explained variance (e.g., 90–95%) and cross-validation performance to pick optimal components.\n",
        "\n",
        "3. Train KNN on PCA-Reduced Data\n",
        "\n",
        "    - Use top PCs as input features.\n",
        "\n",
        "    - Tune k, metric (Euclidean/Manhattan), and weights with cross-validation.\n",
        "\n",
        "4. Evaluate the Model\n",
        "\n",
        "    - Apply stratified cross-validation to avoid overfitting.\n",
        "\n",
        "    - Use accuracy, F1-score, AUC (or AUPRC for imbalanced data).\n",
        "\n",
        "    - Optionally validate on independent test data.\n",
        "\n",
        "5. Justify to Stakeholders\n",
        "\n",
        "    - PCA reduces noise and overfitting risk.\n",
        "\n",
        "    - KNN is simple, transparent, and data-driven.\n",
        "\n",
        "    - The pipeline generalizes better, is interpretable, and suitable for real-world biomedical data with small sample sizes."
      ],
      "metadata": {
        "id": "WXDd8sxzXzQ2"
      }
    }
  ]
}